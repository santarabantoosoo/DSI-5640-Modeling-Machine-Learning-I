---
title: "hw07"
author: "Hamza"
date: "5/1/2022"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(keras)
```



```{r}
library('rgl')

library('nnet')

load("mixture.example.RData")
dat <- mixture.example
ddat <- data.frame(y=dat$y, x1=dat$x[,1], x2=dat$x[,2])



## create 3D plot of mixture data
plot_mixture_data <- function(dat=mixture.example, showtruth=FALSE) {
  plot(dat$xnew[,1], dat$xnew[,2], type = 'n',
       xlab="x1", ylab="x2",
       axes=FALSE )

## plot points and bounding box
x1r <- range(dat$px1)
x2r <- range(dat$px2)
pts <- plot(dat$x[,1], dat$x[,2], 
              type="p",
              col=ifelse(dat$y, "orange", "blue"))
lns <- lines(x1r[c(1,2,2,1,1)], x2r[c(1,1,2,2,1)])

## draw Bayes (True) classification boundary in blue
dat$probm <- with(dat, matrix(prob, length(px1), length(px2)))
dat$cls <- with(dat, contourLines(px1, px2, probm, levels=0.5))
pls0 <- lapply(dat$cls, function(p) lines(p$x, p$y, color="blue"))

## compute probabilities plot classification boundary
## associated with local linear logistic regression
probs.loc <- 
  apply(dat$xnew, 1, function(x0) {
    ## smoothing parameter
    l <- 1/2
    ## compute (Gaussian) kernel weights
    d <- colSums((rbind(ddat$x1, ddat$x2) - x0)^2)
    k <- exp(-d/2/l^2)
    ## local fit at x0
    fit <- suppressWarnings(glm(y ~ x1 + x2, data=ddat, weights=k,
                                family=binomial(link="logit")))
    ## predict at x0
    as.numeric(predict(fit, type="response", newdata=as.data.frame(t(x0))))
  })

dat$probm.loc <- with(dat, matrix(probs.loc, length(px1), length(px2)))
dat$cls.loc <- with(dat, contourLines(px1, px2, probm.loc, levels=0.5))
pls <- lapply(dat$cls.loc, function(p) lines3d(p$x, p$y,))

## plot probability surface and decision plane
sfc <- surface3d(dat$px1, dat$px2, probs.loc, alpha=1.0,
                 color="gray", specular="gray")
qds <- quads3d(x1r[c(1,2,2,1)], x2r[c(1,1,2,2)], 0.5, alpha=0.4,
               color="gray", lit=FALSE)
}

plot_nnet_predictions <- function(fit, dat=mixture.example) {
  
  ## create figure
  plot_mixture_data()

  ## compute predictions from nnet
  probs <- predict(fit, dat$xnew, type="raw")[,1]
  probm <- matrix(probs, length(dat$px1), length(dat$px2))
  cls <- contourLines(dat$px1, dat$px2, probm, levels=0.5)
  rslt <- sapply(cls, lines, col='black')
}



## plot data and 'true' probability surface
plot_mixture_data(showtruth=TRUE)
```

## fit single hidden layer, fully connected NN 
## 10 hidden nodes

```{r}

fit <- nnet(x=dat$x, y=dat$y, size=10, entropy=TRUE, decay=0)
plot_nnet_predictions(fit)


```

## count total parameters
## hidden layer w/10 nodes x (2 + 1) input nodes = 30 
## output layer w/1 node x (10 + 1) hidden nodes = 11
## 41 parameters total

```{r}

length(fit$wts)

# 3 hidden nodes
 nnet(x=dat$x, y=dat$y, size=3, entropy=TRUE, decay=0) %>%
 plot_nnet_predictions
```

```{r}


# 10 hidden nodes with weight decay
nnet(x=dat$x, y=dat$y, size=10, entropy=TRUE, decay=0.02) %>%
  plot_nnet_predictions
```

```{r}
# 3 hidden nodes with weight decay
nnet(x=dat$x, y=dat$y, size=3, entropy=TRUE, decay=0.02) %>%
  plot_nnet_predictions

```





```{r}

model <- keras_model_sequential()
model %>%
  layer_dense(units = 30, activation = 'relu', input_shape = c(2))%>%
  layer_dense(units = 10, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')
```


```{r}
model %>% compile(
  optimizer = 'adam', 
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

```

```{r}
model %>% fit(dat$x, dat$y, epochs = 5, verbose = 2)

```
Predictions are quite similar to the nnet package 


Hw06
================
Hamza
4/17/2022

``` r
library(scorecardModelUtils)
library(tidyverse)
```

    ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──

    ## ✓ ggplot2 3.3.5     ✓ purrr   0.3.4
    ## ✓ tibble  3.1.6     ✓ dplyr   1.0.7
    ## ✓ tidyr   1.1.4     ✓ stringr 1.4.0
    ## ✓ readr   2.1.1     ✓ forcats 0.5.1

    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(randomForest)
```

    ## randomForest 4.7-1

    ## Type rfNews() to see new features/changes/bug fixes.

    ## 
    ## Attaching package: 'randomForest'

    ## The following object is masked from 'package:dplyr':
    ## 
    ##     combine

    ## The following object is masked from 'package:ggplot2':
    ## 
    ##     margin

``` r
library(mlbench)
library(caret)
```

    ## Loading required package: lattice

    ## 
    ## Attaching package: 'caret'

    ## The following object is masked from 'package:purrr':
    ## 
    ##     lift

``` r
train <- read.csv('vowel.train' )
test <- read.csv('vowel.test')

train <- train[,2:ncol(train)]
```

``` r
train$y <- factor(train$y)

X_train <-train %>% 
  select(-y)
y_train <- train$y

X_test <- test %>% 
  select(-y)

y_test <- test$y


basic_model = randomForest(data = train, x = X_train, y_train)
```

``` r
for (i in c(1, 5, 10, 20, 40, 80)){
control <- trainControl(method="repeatedcv", number=5, search="grid")

tunegrid <- expand.grid(.mtry=c(2:6))
rf_gridsearch <- train(y~., data=train, method="rf", metric= 'Accuracy', tuneGrid=tunegrid, trControl=control, nodesize = i)
print(rf_gridsearch)
print("Node size ")
print (i)
}
```

    ## Random Forest 
    ## 
    ## 528 samples
    ##  10 predictor
    ##  11 classes: '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold, repeated 1 times) 
    ## Summary of sample sizes: 421, 422, 422, 423, 424 
    ## Resampling results across tuning parameters:
    ## 
    ##   mtry  Accuracy   Kappa    
    ##   2     0.9640201  0.9604058
    ##   3     0.9696275  0.9665779
    ##   4     0.9545132  0.9499428
    ##   5     0.9526264  0.9478668
    ##   6     0.9469301  0.9416011
    ## 
    ## Accuracy was used to select the optimal model using the largest value.
    ## The final value used for the model was mtry = 3.
    ## [1] "Node size "
    ## [1] 1
    ## Random Forest 
    ## 
    ## 528 samples
    ##  10 predictor
    ##  11 classes: '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold, repeated 1 times) 
    ## Summary of sample sizes: 423, 423, 423, 419, 424 
    ## Resampling results across tuning parameters:
    ## 
    ##   mtry  Accuracy   Kappa    
    ##   2     0.9603828  0.9564103
    ##   3     0.9584597  0.9542934
    ##   4     0.9508040  0.9458673
    ##   5     0.9508556  0.9459290
    ##   6     0.9358272  0.9293984
    ## 
    ## Accuracy was used to select the optimal model using the largest value.
    ## The final value used for the model was mtry = 2.
    ## [1] "Node size "
    ## [1] 5
    ## Random Forest 
    ## 
    ## 528 samples
    ##  10 predictor
    ##  11 classes: '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold, repeated 1 times) 
    ## Summary of sample sizes: 420, 423, 420, 426, 423 
    ## Resampling results across tuning parameters:
    ## 
    ##   mtry  Accuracy   Kappa    
    ##   2     0.9336788  0.9270410
    ##   3     0.9185963  0.9104342
    ##   4     0.9128820  0.9041636
    ##   5     0.9035730  0.8939131
    ##   6     0.8978556  0.8876317
    ## 
    ## Accuracy was used to select the optimal model using the largest value.
    ## The final value used for the model was mtry = 2.
    ## [1] "Node size "
    ## [1] 10
    ## Random Forest 
    ## 
    ## 528 samples
    ##  10 predictor
    ##  11 classes: '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold, repeated 1 times) 
    ## Summary of sample sizes: 422, 422, 424, 424, 420 
    ## Resampling results across tuning parameters:
    ## 
    ##   mtry  Accuracy   Kappa    
    ##   2     0.8347807  0.8181922
    ##   3     0.8100656  0.7909945
    ##   4     0.8008816  0.7808861
    ##   5     0.7725757  0.7497271
    ##   6     0.7744638  0.7517982
    ## 
    ## Accuracy was used to select the optimal model using the largest value.
    ## The final value used for the model was mtry = 2.
    ## [1] "Node size "
    ## [1] 20
    ## Random Forest 
    ## 
    ## 528 samples
    ##  10 predictor
    ##  11 classes: '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold, repeated 1 times) 
    ## Summary of sample sizes: 426, 420, 425, 421, 420 
    ## Resampling results across tuning parameters:
    ## 
    ##   mtry  Accuracy   Kappa    
    ##   2     0.7063699  0.6768649
    ##   3     0.6818843  0.6500068
    ##   4     0.6857090  0.6542196
    ##   5     0.6759673  0.6434717
    ##   6     0.6664176  0.6329804
    ## 
    ## Accuracy was used to select the optimal model using the largest value.
    ## The final value used for the model was mtry = 2.
    ## [1] "Node size "
    ## [1] 40
    ## Random Forest 
    ## 
    ## 528 samples
    ##  10 predictor
    ##  11 classes: '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (5 fold, repeated 1 times) 
    ## Summary of sample sizes: 422, 423, 420, 425, 422 
    ## Resampling results across tuning parameters:
    ## 
    ##   mtry  Accuracy   Kappa    
    ##   2     0.5571619  0.5129499
    ##   3     0.5478616  0.5026781
    ##   4     0.5459368  0.5004572
    ##   5     0.5249803  0.4773662
    ##   6     0.5135845  0.4648314
    ## 
    ## Accuracy was used to select the optimal model using the largest value.
    ## The final value used for the model was mtry = 2.
    ## [1] "Node size "
    ## [1] 80

Thus, we go for mtry 2 and nodes 1

``` r
# optimized model 

optim_model = randomForest(data = train, x = X_train, y_train, mtry = 2, nodesize = 1, importance=TRUE)
```

## prediction

``` r
predict(optim_model, test)
```

    ##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 
    ##   1   2   3   4   5   6   7   8   9   1  11   1   2   3   4   5   6   7   8   9 
    ##  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 
    ##   1   2   1   2   2   4   5  11   7   8   9   3  11   1   2   3   3   7  11   7 
    ##  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 
    ##   8   9   3  11   1   2   2   3   7  11   7   8   9   3   9   1   2   2   3   7 
    ##  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 
    ##  11   7   8   9   3   9   2   2   3   4   5   5   7   8   9  10   9   2   2   3 
    ##  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 
    ##   4   5   5   7   8   9  10   9   2   9   3   4   5   5   7   8   9  10   9   2 
    ## 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 
    ##   9   3   4   5   5   7   8   9  10   9   2   9   3   4   5   6   7   8  10  10 
    ## 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 
    ##   9   1   9   3   4   5   6   7   8  10  10   9   1   3   3   4   5   5   5   8 
    ## 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 
    ##   7  10   9   1   3   3   4   5   5   7   8   8  10   9   1   3   3   4   5   5 
    ## 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 
    ##   7   8   8  10   9   1   3   6   4   5   5   5   8   7  10   9   1   2   3   4 
    ## 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 
    ##   5  11   5   8   9  10   9   1   2  11  11   5  11   5   8   9  10   4   1   3 
    ## 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 
    ##   3   4   6   6   7   9   9   2  11   1   3   3   4   6   6   7   8   9   2  11 
    ## 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 
    ##   1   3   3   4   6   6   7   8   8   2  11   1   3   3   4   6   6   7   8   8 
    ## 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 
    ##  10  11   1   3   3   4   5  11   7   8   8  10   9   1   3   3   4   5  11   7 
    ## 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 
    ##   8   9  10  11   1   2   4   4   4   6   5   7   9  10   6   1   2   3   4   4 
    ## 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 
    ##   6   5   7   7  10   6   1   2   3   4   4   6   5   7   7   9   9   1   2   3 
    ## 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 
    ##   4   6   6   7   7   7  10   7   1   3   3   4   6   6   5   7   9  10   7   1 
    ## 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 
    ##   3   3   4   6   6   5   7   9  10   7   1   2   3   6   6   6   2   9  11   2 
    ## 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 
    ##  11   1   3   4   6   6   6   2   8  11   2  11   1   2   4   6   6   6   9   9 
    ## 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 
    ##  11   2  11   1   2   4   6   6   3   6   9  11   2   6   1   2   3   6   6   3 
    ## 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 
    ##   9   9   2   2  11   1   2   3   6   6   6   6   9   2   2  11   3   2   4   4 
    ## 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 
    ##   6   6   7   8   9   2  11   2   2   3   6  11   6   7   8   9   2  11   2   2 
    ## 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 
    ##   3   6  11   6   7   8   9   2  11   2   2   3   4   6   6   7   8   9   2  11 
    ## 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 
    ##   2   2   3   4   6   6   7   8   9   2   6   2   2   3   4   6   6   7   8   9 
    ## 461 462 
    ##  10   6 
    ## Levels: 1 2 3 4 5 6 7 8 9 10 11

## misclassification rate

``` r
1-mean(margin(optim_model))
```

    ## [1] 0.4387423

``` r
prediction_for_table <- predict(optim_model, X_test)
table(observed= y_test, predicted=prediction_for_table)
```

    ##         predicted
    ## observed  1  2  3  4  5  6  7  8  9 10 11
    ##       1  31 10  1  0  0  0  0  0  0  0  0
    ##       2   0 25 13  0  0  0  0  0  4  0  0
    ##       3   0  3 32  5  0  1  0  0  0  0  1
    ##       4   0  0  3 30  0  8  0  0  0  0  1
    ##       5   0  0  0  3 17 17  3  0  0  0  2
    ##       6   0  0  2  0  8 24  0  0  0  0  8
    ##       7   0  2  0  0  9  2 27  0  2  0  0
    ##       8   0  0  0  0  0  0  6 31  5  0  0
    ##       9   0  2  0  0  0  0  5  5 24  2  4
    ##       10  2 14  4  0  0  0  0  0  1 21  0
    ##       11  0  1  0  1  0  5  3  0 15  0 17
